{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "893652a8-1d09-4311-823c-8c054a8ec22c",
   "metadata": {},
   "source": [
    "# **Amazon EC2 for NLP Workloads**\n",
    "\n",
    "## **1. Introduction to Amazon EC2 for NLP**\n",
    "Amazon Elastic Compute Cloud (**EC2**) is a cloud computing service that provides **scalable and flexible virtual machines** for various workloads, including **Natural Language Processing (NLP)**. NLP tasks such as **text classification, Named Entity Recognition (NER), machine translation, and large language model (LLM) training** require significant computational resources, making **EC2 an ideal choice** for hosting and scaling NLP workloads.\n",
    "\n",
    "EC2 allows **on-demand, spot, and reserved instances**, enabling cost optimization while maintaining performance. By choosing the right **instance type (CPU vs. GPU)**, NLP workloads can be efficiently processed based on the required computing power.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. EC2 Instance Types for NLP Workloads**\n",
    "Different NLP tasks require varying levels of computational resources. AWS offers a wide range of **EC2 instance types**, including CPU-based, GPU-based, and high-memory instances.\n",
    "\n",
    "### **a) CPU-Based Instances (General NLP Tasks)**\n",
    "For lightweight NLP tasks such as **text preprocessing, TF-IDF vectorization, tokenization, and rule-based models**, CPU-based instances are sufficient.\n",
    "\n",
    "| **Instance Type** | **vCPUs** | **RAM (GB)** | **Use Case** |\n",
    "|------------------|-----------|-------------|--------------|\n",
    "| **t3.medium** | 2 | 4 | Small-scale NLP tasks, inference |\n",
    "| **m5.large** | 2 | 8 | Moderate text processing, model inference |\n",
    "| **c5.xlarge** | 4 | 8 | Faster processing, sentiment analysis |\n",
    "| **r5.large** | 2 | 16 | Handling large text datasets |\n",
    "\n",
    "**Use Case Example:**  \n",
    "- Running **NLTK**, **spaCy**, or **scikit-learn** for text preprocessing.\n",
    "- Hosting a small-scale **Flask-based NLP API** for inference.\n",
    "\n",
    "---\n",
    "\n",
    "### **b) GPU-Based Instances (Deep Learning NLP Models)**\n",
    "For training and fine-tuning **deep learning models like BERT, GPT, and LLaMA**, GPUs significantly accelerate matrix operations and parallel computations.\n",
    "\n",
    "| **Instance Type** | **GPUs** | **vCPUs** | **RAM (GB)** | **Use Case** |\n",
    "|------------------|---------|-----------|-------------|--------------|\n",
    "| **g4dn.xlarge** | 1x NVIDIA T4 | 4 | 16 | Small-scale model training |\n",
    "| **p3.2xlarge** | 1x NVIDIA V100 | 8 | 61 | Fine-tuning transformers |\n",
    "| **p4d.24xlarge** | 8x NVIDIA A100 | 96 | 1,152 | Large-scale model training |\n",
    "| **g5.12xlarge** | 4x NVIDIA A10G | 48 | 192 | Medium-sized LLM fine-tuning |\n",
    "\n",
    "**Use Case Example:**\n",
    "- **Fine-tuning BERT on a custom medical dataset** using **PyTorch** or **TensorFlow**.\n",
    "- **Training transformer-based text classification models** on large datasets.\n",
    "\n",
    "**Code Example for Running BERT Fine-Tuning on EC2:**\n",
    "```python\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Compute Power Requirements for NLP Tasks**\n",
    "The choice of EC2 instance depends on **model size, dataset complexity, and processing speed** requirements.\n",
    "\n",
    "| **NLP Task** | **Compute Requirement** | **Best Instance Type** |\n",
    "|-------------|------------------------|------------------------|\n",
    "| **Text Preprocessing** | Low | `t3.medium`, `m5.large` |\n",
    "| **Sentiment Analysis (ML-based)** | Moderate | `c5.xlarge` |\n",
    "| **Named Entity Recognition (NER)** | High (Deep Learning) | `g4dn.xlarge`, `p3.2xlarge` |\n",
    "| **Fine-Tuning Transformers** | Very High | `p4d.24xlarge`, `g5.12xlarge` |\n",
    "| **Training LLMs (GPT, BERT, LLaMA)** | Extreme | `p4d.24xlarge`, Multi-GPU setup |\n",
    "\n",
    "**Example: Running Sentiment Analysis on EC2**\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load pre-trained sentiment analysis model\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Analyze text\n",
    "result = classifier(\"I love using Amazon EC2 for NLP!\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Accelerating NLP Training with GPUs**\n",
    "### **Why GPUs for NLP?**\n",
    "- **Parallelism:** NLP models involve **large matrix multiplications**, which GPUs handle efficiently.\n",
    "- **Tensor Processing:** Frameworks like **TensorFlow and PyTorch** optimize operations for GPUs.\n",
    "- **Memory Optimization:** Training large models like **BERT and GPT** requires **high VRAM**.\n",
    "\n",
    "### **Optimizing GPU Performance on EC2**\n",
    "- Use **`torch.cuda.amp`** for **mixed-precision training** to reduce memory usage.\n",
    "- Implement **gradient checkpointing** to save memory.\n",
    "- Distribute training across multiple GPUs using **`DataParallel`**.\n",
    "\n",
    "**Example: Using Multiple GPUs for NLP Training**\n",
    "```python\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Load model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Use multiple GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "model.to(\"cuda\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5. EC2 Auto Scaling for NLP Workloads**\n",
    "EC2 Auto Scaling dynamically adjusts the number of instances based on workload demand.\n",
    "\n",
    "### **Use Cases for NLP**\n",
    "- **Handling bursts in NLP inference**: Scale up when many users request model predictions.\n",
    "- **Scaling down during idle periods**: Saves costs by reducing unused instances.\n",
    "- **Load balancing across multiple NLP APIs**: Ensures availability during high traffic.\n",
    "\n",
    "### **How Auto Scaling Works**\n",
    "- **Target Tracking Scaling**: Increases instances when CPU or memory usage crosses a threshold.\n",
    "- **Scheduled Scaling**: Scales up during peak hours (e.g., a chatbot receiving queries).\n",
    "- **Spot Fleet Scaling**: Uses cost-effective **spot instances** for non-urgent NLP tasks.\n",
    "\n",
    "### **Auto Scaling Example for NLP API**\n",
    "#### **1. Create an Auto Scaling Group**\n",
    "```bash\n",
    "aws autoscaling create-auto-scaling-group --auto-scaling-group-name NLP-AutoScaling \\\n",
    "  --launch-template LaunchTemplateId=lt-12345 \\\n",
    "  --min-size 1 --max-size 5 --desired-capacity 2\n",
    "```\n",
    "\n",
    "#### **2. Set Target Tracking Policy (Scale Based on CPU Usage)**\n",
    "```bash\n",
    "aws autoscaling put-scaling-policy --auto-scaling-group-name NLP-AutoScaling \\\n",
    "  --policy-name NLP-CPU-Scaling --policy-type TargetTrackingScaling \\\n",
    "  --target-tracking-configuration file://target-tracking.json\n",
    "```\n",
    "\n",
    "#### **3. Sample JSON for Target Tracking**\n",
    "```json\n",
    "{\n",
    "  \"PredefinedMetricSpecification\": {\n",
    "    \"PredefinedMetricType\": \"ASGAverageCPUUtilization\"\n",
    "  },\n",
    "  \"TargetValue\": 60.0,\n",
    "  \"ScaleOutCooldown\": 300,\n",
    "  \"ScaleInCooldown\": 300\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Conclusion**\n",
    "Amazon EC2 provides **scalability, flexibility, and compute power** for NLP workloads. By selecting the right **CPU or GPU instance types**, developers can efficiently **train, fine-tune, and deploy NLP models**. Additionally, **EC2 Auto Scaling ensures optimal performance and cost-efficiency** for NLP-based applications.\n",
    "\n",
    "### **Key Takeaways**\n",
    "✔ **Choose CPU-based instances for light NLP tasks** (e.g., preprocessing, sentiment analysis).  \n",
    "✔ **Use GPU-based instances for deep learning models** (e.g., transformers, LLMs).  \n",
    "✔ **Optimize training performance** using **multi-GPU parallelism**.  \n",
    "✔ **Leverage Auto Scaling** to handle variable NLP workloads dynamically.  \n",
    "✔ **Use EC2 Spot Instances** for cost-effective NLP training and inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088de915-1052-4c14-a44c-fb00bed05347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
