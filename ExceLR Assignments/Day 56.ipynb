{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d48104-ef1e-4c9f-b9fb-d117f47eacaf",
   "metadata": {},
   "source": [
    "### **Amazon S3 for NLP Applications**  \n",
    "\n",
    "#### **1. What is Amazon S3?**  \n",
    "Amazon Simple Storage Service (Amazon S3) is a **scalable, secure, and durable cloud storage service** offered by AWS. It is designed to store and retrieve **any amount of data at any time** from anywhere on the internet. S3 provides an object storage architecture, where data is stored in **buckets** as objects, each with a unique key.  \n",
    "\n",
    "For **Natural Language Processing (NLP) tasks**, Amazon S3 is commonly used to **store, manage, and process large text datasets** required for training and inference in NLP models.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Using Amazon S3 for NLP Tasks**  \n",
    "\n",
    "Amazon S3 is useful for various NLP-related tasks, such as:  \n",
    "\n",
    "- **Storing Large Text Datasets:** NLP models require vast amounts of textual data. S3 can store **corpora, preprocessed text, embeddings, and trained models** efficiently.  \n",
    "- **Data Preprocessing & Cleaning:** Text files stored in S3 can be **loaded, processed, and cleaned** before training models.  \n",
    "- **Model Training Pipelines:** Machine learning workflows, including training **Transformer models like BERT or GPT**, often fetch data from S3.  \n",
    "- **Serving NLP Models:** Trained NLP models (e.g., weights of **fine-tuned Hugging Face models**) can be stored and retrieved for real-time inference.  \n",
    "- **Backup & Versioning:** NLP datasets and models can be versioned, backed up, and restored when needed.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3. S3 Storage Classes**  \n",
    "\n",
    "Amazon S3 provides **different storage classes** based on access frequency and cost-effectiveness. Choosing the right storage class is crucial for optimizing **NLP data storage**.  \n",
    "\n",
    "| **Storage Class**           | **Use Case**                                          | **Retrieval Time** | **Cost**      |\n",
    "|----------------------------|------------------------------------------------------|--------------------|--------------|\n",
    "| **S3 Standard**            | Frequently accessed NLP datasets & model weights   | Milliseconds       | High         |\n",
    "| **S3 Intelligent-Tiering** | Auto-moves data to low-cost tiers if not accessed  | Milliseconds       | Moderate     |\n",
    "| **S3 Standard-IA**         | Infrequently accessed datasets & backups           | Milliseconds       | Lower        |\n",
    "| **S3 One Zone-IA**         | Rarely accessed NLP data (single-region storage)   | Milliseconds       | Lower        |\n",
    "| **S3 Glacier**             | Archive of old datasets or model checkpoints       | Minutes-Hours      | Very Low     |\n",
    "| **S3 Glacier Deep Archive**| Long-term storage (years) for historical datasets  | Hours              | Lowest       |\n",
    "\n",
    "For NLP:  \n",
    "- **Raw datasets** → S3 Standard  \n",
    "- **Processed datasets** → S3 Standard-IA  \n",
    "- **Trained model weights (less frequently used)** → S3 Glacier  \n",
    "\n",
    "---\n",
    "\n",
    "### **4. Storing and Retrieving Large NLP Datasets**  \n",
    "\n",
    "#### **Storing Large Text Data in S3**  \n",
    "To store an NLP dataset (e.g., Wikipedia corpus, Common Crawl, medical text records) in S3:  \n",
    "\n",
    "```python\n",
    "import boto3\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Upload a dataset file\n",
    "s3.upload_file(\"large_text_data.txt\", \"my-nlp-bucket\", \"datasets/large_text_data.txt\")\n",
    "print(\"Dataset uploaded successfully\")\n",
    "```\n",
    "\n",
    "#### **Retrieving Data for NLP Processing**  \n",
    "To fetch an NLP dataset from S3:  \n",
    "\n",
    "```python\n",
    "# Download file from S3\n",
    "s3.download_file(\"my-nlp-bucket\", \"datasets/large_text_data.txt\", \"local_text_data.txt\")\n",
    "print(\"Dataset downloaded successfully\")\n",
    "```\n",
    "\n",
    "For **large datasets**, AWS **S3 Select** can retrieve only relevant parts of a dataset, improving efficiency.\n",
    "\n",
    "```python\n",
    "response = s3.select_object_content(\n",
    "    Bucket=\"my-nlp-bucket\",\n",
    "    Key=\"datasets/large_text_data.csv\",\n",
    "    ExpressionType=\"SQL\",\n",
    "    Expression=\"SELECT * FROM S3Object WHERE text LIKE '%machine learning%'\",\n",
    "    InputSerialization={\"CSV\": {}},\n",
    "    OutputSerialization={\"CSV\": {}},\n",
    ")\n",
    "```\n",
    "This reduces bandwidth costs and speeds up queries.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Security Mechanisms for NLP Data in S3**  \n",
    "\n",
    "Security is crucial when storing NLP datasets, especially for **medical records, financial data, or confidential corpora**.\n",
    "\n",
    "#### **1. Access Control & IAM Roles**  \n",
    "- Use **IAM policies** to restrict access to only authorized users.  \n",
    "- Example: Restrict access to a specific user for NLP dataset downloads.  \n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Effect\": \"Allow\",\n",
    "    \"Action\": \"s3:GetObject\",\n",
    "    \"Resource\": \"arn:aws:s3:::my-nlp-bucket/datasets/*\",\n",
    "    \"Principal\": { \"AWS\": \"arn:aws:iam::123456789012:user/NLPUser\" }\n",
    "}\n",
    "```\n",
    "\n",
    "#### **2. Encryption**  \n",
    "- **Server-Side Encryption (SSE-S3, SSE-KMS)** ensures stored NLP data is encrypted.  \n",
    "- **Client-Side Encryption** encrypts data before uploading.\n",
    "\n",
    "```python\n",
    "s3.put_object(Bucket=\"my-nlp-bucket\", Key=\"datasets/text_data.txt\", Body=data, ServerSideEncryption=\"AES256\")\n",
    "```\n",
    "\n",
    "#### **3. Bucket Policies & Public Access Blocking**  \n",
    "- Prevent unauthorized public access by enforcing strict **bucket policies**.  \n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Effect\": \"Deny\",\n",
    "    \"Principal\": \"*\",\n",
    "    \"Action\": \"s3:*\",\n",
    "    \"Resource\": \"arn:aws:s3:::my-nlp-bucket/*\",\n",
    "    \"Condition\": { \"Bool\": { \"aws:SecureTransport\": \"false\" } }\n",
    "}\n",
    "```\n",
    "\n",
    "#### **4. Versioning & Backup**  \n",
    "- **Enable versioning** to keep track of changes in large NLP datasets.  \n",
    "- Restore previous versions in case of accidental deletion.\n",
    "\n",
    "```python\n",
    "s3.put_bucket_versioning(\n",
    "    Bucket=\"my-nlp-bucket\",\n",
    "    VersioningConfiguration={\"Status\": \"Enabled\"}\n",
    ")\n",
    "```\n",
    "\n",
    "#### **5. Monitoring & Logging**  \n",
    "- Enable **AWS CloudTrail** and **S3 Access Logs** to track access and modifications.\n",
    "\n",
    "```python\n",
    "s3.put_bucket_logging(\n",
    "    Bucket=\"my-nlp-bucket\",\n",
    "    BucketLoggingStatus={\n",
    "        \"LoggingEnabled\": {\n",
    "            \"TargetBucket\": \"log-bucket\",\n",
    "            \"TargetPrefix\": \"s3-logs/\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Processing Large Text Datasets in S3 for NLP**  \n",
    "\n",
    "S3 integrates with **AWS compute services** to process large NLP datasets efficiently.\n",
    "\n",
    "#### **1. Using AWS Lambda for Preprocessing**  \n",
    "- AWS Lambda can automatically **process incoming NLP datasets** (e.g., text cleaning, tokenization).  \n",
    "\n",
    "```python\n",
    "def lambda_handler(event, context):\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    bucket = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]\n",
    "    key = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]\n",
    "    \n",
    "    # Download file\n",
    "    s3.download_file(bucket, key, \"/tmp/temp_file.txt\")\n",
    "    \n",
    "    # Process text file\n",
    "    with open(\"/tmp/temp_file.txt\", \"r\") as f:\n",
    "        text = f.read().lower()  # Convert to lowercase\n",
    "    \n",
    "    # Upload processed file\n",
    "    s3.upload_file(\"/tmp/temp_file.txt\", bucket, \"processed/\" + key)\n",
    "    return \"Processing complete\"\n",
    "```\n",
    "\n",
    "#### **2. Using Amazon SageMaker for NLP Model Training**  \n",
    "- NLP models can be trained directly on S3-stored datasets using **Amazon SageMaker**.  \n",
    "\n",
    "```python\n",
    "from sagemaker import Session\n",
    "sagemaker_session = Session()\n",
    "s3_uri = \"s3://my-nlp-bucket/datasets/large_text_data.txt\"\n",
    "\n",
    "# Load dataset from S3 into a SageMaker training job\n",
    "s3_input = sagemaker_session.upload_data(\"local_text_data.txt\", bucket=\"my-nlp-bucket\")\n",
    "```\n",
    "\n",
    "#### **3. Using AWS Glue for Data ETL**  \n",
    "- AWS Glue can be used to **transform unstructured text data** into structured formats (e.g., CSV, JSON) for NLP pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**  \n",
    "Amazon S3 is a powerful storage solution for **NLP applications**, enabling **scalability, security, and seamless integration** with AWS services. It supports **storage, preprocessing, model training, and deployment** while ensuring **data security and cost optimization**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda1bac8-09e1-41f0-b6b7-413ab45937a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
